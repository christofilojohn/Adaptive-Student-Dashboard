# ğŸ§  Adaptive Dashboard

A local-first AI dashboard powered by **Phi-3.5-mini** running entirely on your hardware.

## Quick Start

### ğŸ macOS (Apple Silicon â€” M1/M2/M3/M4)

The fastest path â€” native Metal GPU acceleration, no Docker needed:

```bash
# 1. Install llama.cpp (one-time)
brew install llama.cpp

# 2. Install Node.js if needed (one-time)
brew install node

# 3. Launch everything
./start.sh
```

The script will:
- âœ… Detect your M-series chip and use Metal GPU
- âœ… Download the model automatically (~2.4 GB, one-time)
- âœ… Start the LLM server + React frontend
- âœ… Open your browser to `http://localhost:5173`

---

### ğŸŸ¢ NVIDIA GPU (Linux / Windows WSL2)

**Option A â€” Docker (recommended, easiest):**

```bash
# Prerequisites: Docker + NVIDIA Container Toolkit
# https://docs.nvidia.com/datacenter/cloud-native/container-toolkit/install-guide.html

# Download the model first
mkdir -p models
docker compose --profile download run download-model

# Launch
docker compose --profile nvidia up
```
â†’ Open `http://localhost:3000`

**Option B â€” Native (faster cold start):**

```bash
# Build llama.cpp with CUDA
./scripts/install-llama.sh

# Install Node.js if needed
# Ubuntu: curl -fsSL https://fnm.vercel.app/install | bash && fnm install 20

# Launch
./start.sh
```

---

### ğŸ”´ AMD GPU (Linux â€” ROCm)

**Option A â€” Docker:**

```bash
# Prerequisites: ROCm drivers installed
# https://rocm.docs.amd.com/projects/install-on-linux/en/latest/

mkdir -p models
docker compose --profile download run download-model
docker compose --profile amd up
```

> If your GPU isn't auto-detected, set `HSA_OVERRIDE_GFX_VERSION` in `.env`:
> ```
> # .env
> HSA_OVERRIDE_GFX_VERSION=10.3.0   # RX 6000 series
> HSA_OVERRIDE_GFX_VERSION=11.0.0   # RX 7000 series
> ```

**Option B â€” Native:**

```bash
./scripts/install-llama.sh   # auto-detects ROCm
./start.sh
```

---

### ğŸ–¥ï¸ CPU-only fallback

Works on any machine, just slower (~3-8 tokens/sec):

```bash
# Docker
docker compose --profile cpu up

# Native
./start.sh    # auto-falls-back to CPU mode
```

---

## Architecture

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚            Your Browser                 â”‚
â”‚         localhost:5173 (dev)            â”‚
â”‚         localhost:3000 (docker)         â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                   â”‚  HTTP / REST
         â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
         â”‚   Vite / Nginx    â”‚  (frontend)
         â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                   â”‚  proxy /v1/*
         â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
         â”‚   llama-server    â”‚  :8080
         â”‚  Phi-3.5-mini     â”‚
         â”‚  OpenAI-compat APIâ”‚
         â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                   â”‚
         â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
         â”‚   Metal / CUDA    â”‚
         â”‚   ROCm / CPU      â”‚
         â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

## Docker Build Details

| Profile   | Base Image              | GPU backend    |
|-----------|-------------------------|----------------|
| `nvidia`  | `nvidia/cuda:12.2`      | CUDA           |
| `amd`     | `rocm/dev-ubuntu-22.04` | HIP/ROCm       |
| `cpu`     | `ubuntu:22.04`          | CPU (AVX2)     |

Build a specific image manually:

```bash
# NVIDIA
docker build --build-arg BUILD_TYPE=cuda -t adaptive-dashboard .

# AMD
docker build --build-arg BUILD_TYPE=rocm -t adaptive-dashboard-rocm .

# CPU
docker build --build-arg BUILD_TYPE=cpu -t adaptive-dashboard-cpu .
```

## File Layout

```
dashboard/
â”œâ”€â”€ start.sh                 â† ğŸš€ Main launcher (all platforms)
â”œâ”€â”€ docker-compose.yml       â† Docker (NVIDIA / AMD / CPU profiles)
â”œâ”€â”€ Dockerfile               â† Multi-stage build
â”œâ”€â”€ .env.example             â† Config template
â”œâ”€â”€ frontend/
â”‚   â”œâ”€â”€ src/App.jsx          â† Your dashboard app
â”‚   â”œâ”€â”€ src/main.jsx
â”‚   â”œâ”€â”€ index.html
â”‚   â”œâ”€â”€ vite.config.js       â† Proxies /v1/* to llama-server
â”‚   â””â”€â”€ package.json
â”œâ”€â”€ scripts/
â”‚   â””â”€â”€ install-llama.sh    â† llama.cpp auto-installer
â””â”€â”€ docker/
    â”œâ”€â”€ supervisord.conf     â† Manages llama-server + nginx
    â””â”€â”€ entrypoint.sh
```

## Changing the Model

The default is `Phi-3.5-mini-instruct-Q4_K_M` (~2.4 GB, great quality/speed).
To use a different model, edit `start.sh`:

```bash
MODEL_FILE="your-model.gguf"
MODEL_REPO="author/repo-name-GGUF"
```

Or for Docker, mount your model and set `MODEL_PATH`:
```yaml
environment:
  - MODEL_PATH=/models/your-model.gguf
```

## Troubleshooting

**LLM not responding:**
```bash
# Check llama-server logs
cat .llm.log
# Or for docker
docker compose logs dashboard-nvidia
```

**GPU not used (NGL=0):**
- macOS: ensure you installed llama.cpp with Metal (`brew install llama.cpp` does this automatically)
- NVIDIA: verify `nvidia-smi` works and CUDA is installed
- AMD: verify `rocm-smi` works; try setting `HSA_OVERRIDE_GFX_VERSION`

**Port conflict:**
Edit `LLM_PORT` or `UI_PORT` at the top of `start.sh`.

**Model download failed:**
```bash
pip install huggingface-hub
huggingface-cli download bartowski/Phi-3.5-mini-instruct-GGUF \
  Phi-3.5-mini-instruct-Q4_K_M.gguf \
  --local-dir ~/.cache/dashboard-models
```
